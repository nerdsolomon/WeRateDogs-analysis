{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting : wrangle_report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project wrangling process started with carefully following the classroom tutorials of which i attempted the various classroom quizzes to better my knowledge on the project i was about to embark on.\n",
    "\n",
    "Then i started making tools decisions of which is where i decided that i would be working with my Jupyter notebook and the i started identifying the various data I'll be using to carry out the project and preparing my notebook by installing the various packages needed to read and gather the data, like tweepy, pandas, matplotlib and as well as registering for a Twitter developer's account where i made requests for the API I will using to download the tweet_json file using the API key.\n",
    "\n",
    "Then i started with the gathering process of all three datasets, from different sources as instructed in the classroom. \n",
    "\n",
    "For the twitter-archive file, i manually downloaded the twitter-archive.csv file from the Udacity classroom where the needed link was provided with a quick click download prompt.\n",
    "\n",
    "For the image prediction  file, i programmatically downloaded the image-prediction.tsv file from the link provided in the classroom which i copied and paste in my notebook and using a few lines of code it was successfully downloaded into my system.\n",
    "\n",
    "For the tweet json file, i first got registered into the Twitter developer's website where if filled various forms and authentication process before i create a new product and request for the API keys. After i have the API information sent to me, i began to  write codes in my script using the tweepy module that i will use to extract the various data in the creating of the tweet_json.txt file, which was read and saved line by line.\n",
    "\n",
    "Then i moved to the next step of assessing all available datasets  which are the now included files sent in this zip. The files are; \n",
    "twitter_archive_enhanced.csv, image_prediction.tsv and tweet_json.txt.\n",
    "\n",
    "The assessing process included visual assessment where i used my mere eyesight to check through the files in an excel app for both data quality issues and Tidiness issues as instructed in the classroom. The i as well assessed the files programmatically using various pandas functions like describe(), isnull(), head() and so on, to also get quality and Tidiness issues.\n",
    "\n",
    "Next was the cleaning process, the cleaning process which includes addressing the various quality issues and Tidiness issues accounted for in the assessment process, where i addressed the issues one after the other by writing codes to fix them and testing them immediately. \n",
    "\n",
    "After the all three datasets have been cleaned according to all issues stated in the assessment process, they were all merged based on their tweet ID  into one big dataset in a csv format which i named twitter_archive_master.csv as instructed in the classroom. \n",
    "\n",
    "For Analysis and visualization, i used the cleaned and merged datasets to run Analysis on three points which are namely; the most favourited tweets, the most popular dog breed and the most used pet names of which i ended up Visualizing the results for the most favourited tweets which ended the project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
